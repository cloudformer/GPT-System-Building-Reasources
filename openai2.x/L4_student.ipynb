{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7531d5-0c22-49ad-9d37-8b08eec7d4e0",
   "metadata": {},
   "source": [
    "# L4: Process Inputs: Chain of Thought Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613f6af-ce1c-49ea-ae99-0d2e3fa3fae1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "In this course, we've provided some code that loads the OpenAI API key for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df1c9e8",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# L4: Chain-of-Thought Reasoning\n",
    "# --------------------------\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 读取本地 .env 文件\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 初始化客户端\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98a05b6",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Helper 函数\n",
    "# --------------------------\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=500):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 2.x SDK 调用 ChatCompletion 接口\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d273f-df72-47e2-a9a6-a8994d742aec",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e66beb-8fb5-4c7b-afa7-13d20ded1d49",
   "metadata": {
    "height": 1558
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型原始输出:\n",
      " ####Step 1: \n",
      "Decide whether the user is asking about a specific product.\n",
      "####Step 2: \n",
      "If so, check if the product is in the available list.\n",
      "####Step 3: \n",
      "List any assumptions the user is making.\n",
      "####Step 4: \n",
      "Verify assumptions against product info.\n",
      "####Step 5: \n",
      "Politely correct incorrect assumptions and answer user.\n",
      "最终用户回答:\n",
      " Step 5: \n",
      "Politely correct incorrect assumptions and answer user.\n",
      "最终用户回答:\n",
      " Step 5: \n",
      "Politely correct incorrect assumptions and answer user.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Chain-of-Thought Prompt\n",
    "# --------------------------\n",
    "delimiter = \"####\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to answer the customer queries.\n",
    "The customer query will be delimited with {delimiter}.\n",
    "\n",
    "Step 1:{delimiter} Decide whether the user is asking about a specific product.\n",
    "Step 2:{delimiter} If so, check if the product is in the available list.\n",
    "Step 3:{delimiter} List any assumptions the user is making.\n",
    "Step 4:{delimiter} Verify assumptions against product info.\n",
    "Step 5:{delimiter} Politely correct incorrect assumptions and answer user.\n",
    "Use {delimiter} to separate every step.\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------\n",
    "# 示例问题\n",
    "# --------------------------\n",
    "user_message = \"by how much is the BlueWave Chromebook more expensive than the TechPro Desktop\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": f\"{delimiter}{user_message}{delimiter}\"}\n",
    "]\n",
    "\n",
    "# 获取模型完整输出\n",
    "response = get_completion_from_messages(messages)\n",
    "print(\"模型原始输出:\\n\", response)\n",
    "\n",
    "# --------------------------\n",
    "# Inner Monologue：提取最终回答\n",
    "# --------------------------\n",
    "try:\n",
    "    final_response = response.split(delimiter)[-1].strip()\n",
    "except Exception:\n",
    "    final_response = \"抱歉，当前无法处理您的请求，请换一个问题。\"\n",
    "\n",
    "print(\"最终用户回答:\\n\", final_response)\n",
    "\n",
    "# --------------------------\n",
    "# 再一个例子\n",
    "# --------------------------\n",
    "user_message2 = \"do you sell tvs\"\n",
    "messages2 = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": f\"{delimiter}{user_message2}{delimiter}\"}\n",
    "]\n",
    "\n",
    "response2 = get_completion_from_messages(messages2)\n",
    "try:\n",
    "    final_response2 = response2.split(delimiter)[-1].strip()\n",
    "except Exception:\n",
    "    final_response2 = \"抱歉，当前无法处理您的请求，请换一个问题。\"\n",
    "\n",
    "print(\"最终用户回答:\\n\", final_response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be1ea0a-a816-4694-8a79-77d985f2e274",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Step 1:\n",
      "Decide whether the user is asking about a specific product.\n",
      "#### Step 2:\n",
      "If so, check if the product is in the available list.\n",
      "#### Step 3:\n",
      "List any assumptions the user is making.\n",
      "#### Step 4:\n",
      "Verify assumptions against product info.\n",
      "#### Step 5:\n",
      "Politely correct incorrect assumptions and answer user.\n"
     ]
    }
   ],
   "source": [
    "user_message = f\"\"\"\n",
    "by how much is the BlueWave Chromebook more expensive \\\n",
    "than the TechPro Desktop\"\"\"\n",
    "\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51afe6d",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####Step 1:\n",
      "Decide whether the user is asking about a specific product.\n",
      "####Step 2:\n",
      "If so, check if the product is in the available list.\n",
      "####Step 3:\n",
      "List any assumptions the user is making.\n",
      "####Step 4:\n",
      "Verify assumptions against product info.\n",
      "####Step 5:\n",
      "Politely correct incorrect assumptions and answer user.\n"
     ]
    }
   ],
   "source": [
    "user_message = f\"\"\"\n",
    "do you sell tvs\"\"\"\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552a4f6-5e65-4d85-9579-5263f720aa10",
   "metadata": {},
   "source": [
    "## Inner Monologue\n",
    "- Since we asked the LLM to separate its reasoning steps by a delimiter, we can hide the chain-of-thought reasoning from the final output that the user sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a825237",
   "metadata": {
    "height": 130
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5:\n",
      "Politely correct incorrect assumptions and answer user.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    final_response = response.split(delimiter)[-1].strip()\n",
    "except Exception as e:\n",
    "    final_response = \"Sorry, I'm having trouble right now, please try asking another question.\"\n",
    "    \n",
    "print(final_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
